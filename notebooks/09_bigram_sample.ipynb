{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sys.path.append('../experiments/')\n",
    "import os\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import sys\n",
    "from os.path import join\n",
    "import datasets\n",
    "from dict_hash import sha256\n",
    "import numpy as np\n",
    "from torch.autograd import grad\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.func import jacfwd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# load model\n",
    "checkpoint = 'gpt2'\n",
    "device = 'cuda'\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "# load dset\n",
    "dset_train = datasets.load_dataset('rotten_tomatoes')['train']\n",
    "dset_train = dset_train.select(np.random.choice(\n",
    "    len(dset_train), size=100, replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groundtruth (eval all bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_logits(prefix, tokenizer, model):\n",
    "    input_ids = tokenizer.encode(prefix, return_tensors=\"pt\").to(model.device)\n",
    "    return model(input_ids=input_ids).logits[:, -1, :].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "get_unigram_logits('The', tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'The most popular words that might appear in a positive movie review are'\n",
    "batch_size = 2048\n",
    "voc_size = model.transformer.wte.weight.shape[0]\n",
    "\n",
    "prefix_embs = tokenizer.encode(prefix, return_tensors=\"pt\")[0]\n",
    "input_ids_bigrams = [\n",
    "    torch.concatenate(\n",
    "        (prefix_embs, torch.LongTensor([i])))\n",
    "    for i in tqdm(range(voc_size))\n",
    "    # for j in range(voc_size)\n",
    "]\n",
    "input_ids_bigrams = torch.stack(input_ids_bigrams).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_logits = np.zeros((voc_size, voc_size))\n",
    "unigram_logits = get_unigram_logits(prefix, tokenizer, model)\n",
    "for i in tqdm(range(0, voc_size, batch_size)):\n",
    "    input_ids = input_ids_bigrams[i:i + batch_size]\n",
    "    bigram_logits[i:i + batch_size] = \\\n",
    "        model(input_ids).logits[:, -1, :].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram1_probs = scipy.special.softmax(unigram_logits).flatten()\n",
    "unigram2_probs = scipy.special.softmax(bigram_logits, axis=1)\n",
    "\n",
    "# multiply each row of unigram2_probs by the corresponding value in unigram_probs\n",
    "bigram_probs = unigram2_probs * unigram1_probs[:, None]\n",
    "assert np.allclose(bigram_probs.sum(axis=1), unigram1_probs)\n",
    "assert np.allclose(bigram_probs.sum(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find top 100 bigrams (row, col) pairs\n",
    "bigram_probs_flat = bigram_probs.flatten()\n",
    "bigram_probs_flat_sorted = np.sort(bigram_probs_flat)[::-1]\n",
    "bigram_probs_flat_sorted_idx = np.argsort(bigram_probs_flat)[::-1]\n",
    "bigram_probs_sorted = bigram_probs_flat_sorted.reshape(bigram_probs.shape)\n",
    "bigram_probs_sorted_idx = np.unravel_index(\n",
    "    bigram_probs_flat_sorted_idx, bigram_probs.shape)\n",
    "bigram_probs_sorted_idx = np.stack(bigram_probs_sorted_idx, axis=1)\n",
    "bigram_probs_sorted_idx = bigram_probs_sorted_idx[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_bigrams = tokenizer.batch_decode(bigram_probs_sorted_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate jacobian-based logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'The most popular words that might appear in a positive movie review are'\n",
    "input_ids = tokenizer.encode(prefix, return_tensors=\"pt\").to(model.device)\n",
    "input_embs = model.transformer.wte(input_ids)\n",
    "# output = model(inputs_embeds=input_embs)\n",
    "\n",
    "\n",
    "def forward_embs(embs):\n",
    "    output = model(inputs_embeds=embs)\n",
    "    return output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embs = input_embs.to('cuda')\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_embs(embs):\n",
    "    # torch.zeros(1, 6, 768).shape\n",
    "    output = model(inputs_embeds=embs)\n",
    "    return output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jac = jacobian(\n",
    "# forward_embs, input_embs, strategy='forward-mode', vectorize=True)\n",
    "jac = jacfwd(forward_embs)(input_embs)\n",
    "jac = jac.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac_input = jac @ model.transformer.wte.weight.T  # output x input\n",
    "jac_input.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac_input = jac_input.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac_input[:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(jac_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take multi-dimensional grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input tensor\n",
    "x = torch.tensor(torch.normal(0, 1, size=(1, 5)), requires_grad=True)\n",
    "\n",
    "# Compute the output of the function\n",
    "param = torch.ones(5, 4)\n",
    "param[0, :] = 2\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return x @ param\n",
    "\n",
    "\n",
    "# Compute the gradient of y with respect to x\n",
    "# gradient = torch.autograd.grad(y, x, torch.ones_like(y))[0]\n",
    "# gradient = jacobian(f, x)[0]\n",
    "gradient = jacfwd(f)(x)\n",
    "\n",
    "print(\"Input x:\", x)\n",
    "print(\"Output y:\", f(x))\n",
    "print(\"Jacob dy/dx:\", gradient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9ff692d44ea03fd8a03facee7621117bbbb82def09bacaacf0a2cbc238b7b91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
